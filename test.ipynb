{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "sys.path.append('core')\n",
    "from flownet import FlowNetS\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from raft import RAFT\n",
    "from utils import flow_viz\n",
    "from utils.utils import InputPadder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Env\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Env\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resnet = resnet50(pretrained = True).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_warp(f_k : torch.Tensor, flow : torch.Tensor):\n",
    "    n, c, h, w = f_k.shape\n",
    "    kernel_size = 2\n",
    "    f_i = torch.zeros_like(f_k)\n",
    "    flo = - F.interpolate(flow, size=(h,w), mode='bilinear', align_corners=False)\n",
    "\n",
    "    for px in range(w):\n",
    "        for py in range(h):\n",
    "            dpx = flo[:, 0:1, py, px]\n",
    "            dpy = flo[:, 1:, py, px]\n",
    "            i, j = torch.floor(py + dpy), torch.floor(px + dpx)\n",
    "            di, dj = py + dpy - i, px + dpx - j\n",
    "            G = torch.concat([di * dj, di * (1 - dj), (1 - di) * dj, (1 - di) * (1 - dj)], dim=1).reshape(n, 1, kernel_size, kernel_size)\n",
    "            # n, c, kernel, kernel\n",
    "            G = G.repeat(1, c, 1, 1).to(DEVICE)\n",
    "            grid = torch.zeros(n, kernel_size, kernel_size, 2).to(DEVICE)\n",
    "            for gy in range(kernel_size):\n",
    "                for gx in range(kernel_size):\n",
    "                    grid[:, gy, gx, 0:1] = 2 * (j + gx) / (w - 1) - 1\n",
    "                    grid[:, gy, gx, 1:] = 2 * (i + gy) / (h - 1) - 1\n",
    "            # n, c, kernel, kernel\n",
    "            patch = F.grid_sample(f_k, grid,  mode='bilinear', padding_mode='zeros', align_corners=True)\n",
    "            f_i[:,:, py, px] = torch.sum(G * patch, dim=(2, 3))\n",
    "\n",
    "    return f_i\n",
    "\n",
    "def feature_aggregation(frames : torch.Tensor, feature_encoder : nn.Module, flow_net : nn.Module, feature_embedding : nn.Module, K = 10):\n",
    "    feature_maps = feature_encoder(frames)\n",
    "    N, C, _, _ = feature_maps.shape\n",
    "    f_i_aggregation_list = []\n",
    "    for i in range(N):\n",
    "        w_list = []\n",
    "        f_list = []\n",
    "        for j in range(max(0, i - K), min(N, i + K + 1)):\n",
    "            pad_frames = torch.cat([frames[j:j+1], frames[i:i+1]], dim=1)\n",
    "            flow_ji = flow_net(pad_frames)\n",
    "            # 1, c, h, w\n",
    "            f_ji = feature_warp(feature_maps[j:j+1], flow_ji)\n",
    "            # 1, emb\n",
    "            f_ji_emb, f_i_emb = feature_embedding(f_ji), feature_embedding(feature_maps[i:i+1])\n",
    "            # 1, 1, 1, 1\n",
    "            w_ji = torch.exp(torch.sum(f_ji_emb * f_i_emb) / (torch.norm(f_ji_emb, p = 2) *  torch.norm(f_i_emb, p = 2))).reshape(1, 1, 1, 1)\n",
    "            # 1, c, 1, 1\n",
    "            w_ji.repeat(1, C, 1, 1)\n",
    "            f_list.append(f_ji)\n",
    "            w_list.append(w_ji)\n",
    "        # 2K, c, h, w\n",
    "        f = torch.concatenate(f_list, dim=0)\n",
    "        # 2K, c, 1, 1\n",
    "        w = torch.concatenate(w_list, dim=0)\n",
    "        # 1, c, h, w\n",
    "        f_i_aggregation = torch.sum(f * w / torch.sum(w), dim = 0, keepdim=True)\n",
    "        f_i_aggregation_list.append(f_i_aggregation)\n",
    "\n",
    "    feature_map_aggregation = torch.concatenate(f_i_aggregation_list)\n",
    "    return feature_map_aggregation\n",
    "\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, model : nn.Module) -> None:\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.feature = nn.Sequential(*list(model.children())[:-2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        return x\n",
    "    \n",
    "class Feature2Class(nn.Module):\n",
    "    def __init__(self, model : nn.Module) -> None:\n",
    "        super(Feature2Class, self).__init__()\n",
    "        self.avgpool = model.avgpool\n",
    "        self.fc = model.fc\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class FeatureEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels) -> None:\n",
    "        super(FeatureEmbedding, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 512, kernel_size=1, stride=1, padding=0),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(512, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "    def forward(self, x : torch.Tensor):\n",
    "        x = self.conv(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.squeeze(-1).squeeze(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlowNetS(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (conv3_1): Sequential(\n",
       "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (conv4_1): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (conv5): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (conv5_1): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (conv6): Sequential(\n",
       "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (conv6_1): Sequential(\n",
       "    (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (deconv5): Sequential(\n",
       "    (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (deconv4): Sequential(\n",
       "    (0): ConvTranspose2d(1026, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (deconv3): Sequential(\n",
       "    (0): ConvTranspose2d(770, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (deconv2): Sequential(\n",
       "    (0): ConvTranspose2d(386, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (predict_flow6): Conv2d(1024, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (predict_flow5): Conv2d(1026, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (predict_flow4): Conv2d(770, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (predict_flow3): Conv2d(386, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (predict_flow2): Conv2d(194, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (upsampled_flow6_to_5): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (upsampled_flow5_to_4): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (upsampled_flow4_to_3): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (upsampled_flow3_to_2): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_encoder = FeatureExtractor(resnet)\n",
    "feature_encoder.to(DEVICE)\n",
    "feature_encoder.eval()\n",
    "feature_embedding = FeatureEmbedding(2048, 2048)\n",
    "feature_embedding.to(DEVICE)\n",
    "feature_embedding.eval()\n",
    "flow_net = FlowNetS(False).to(DEVICE)\n",
    "flow_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = torch.randn(5, 3, 224, 224).to(DEVICE)\n",
    "f = feature_aggregation(frames, feature_encoder, flow_net, feature_embedding, K = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f():\n",
    "    return 1,2,3\n",
    "\n",
    "a = f()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "l1 = [1,2,3]\n",
    "l2 = ['a', 'b', 'c']\n",
    "for x, a in zip(l1, l2):\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
