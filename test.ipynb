{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "sys.path.append('core')\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from raft import RAFT\n",
    "from utils import flow_viz\n",
    "from utils.utils import InputPadder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Env\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Env\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resnet = resnet50(pretrained = True).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n"
     ]
    }
   ],
   "source": [
    "for name, _ in resnet.named_children():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, model : nn.Module) -> None:\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.feature = nn.Sequential(*list(model.children())[:-2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow_field (img1, img2, model, iters = 20):\n",
    "    _, flow_up = model(img1, img2, iters = 20, test_mode = True)\n",
    "    # (1, 2, H, W)\n",
    "    return flow_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(imfile):\n",
    "    img = np.array(Image.open(imfile)).astype(np.uint8)\n",
    "    img = torch.from_numpy(img).permute(2, 0, 1).float()\n",
    "    return img[None].to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_warp(f_k : torch.Tensor, flow : torch.Tensor):\n",
    "    n, c, h, w = f_k.shape\n",
    "    kernel_size = 2\n",
    "    f_i = torch.zeros_like(f_k)\n",
    "    flo = F.interpolate(flow, size=(h,w), mode='bilinear', align_corners=False)\n",
    "\n",
    "    for px in range(w):\n",
    "        for py in range(h):\n",
    "            dpx = flo[:, 0:1, py, px]\n",
    "            dpy = flo[:, 1:, py, px]\n",
    "            i, j = torch.floor(py + dpy), torch.floor(px + dpx)\n",
    "            di, dj = py + dpy - i, px + dpx - j\n",
    "            G = torch.concat([di * dj, di * (1 - dj), (1 - di) * dj, (1 - di) * (1 - dj)], dim=1).reshape(n, 1, kernel_size, kernel_size)\n",
    "            # n, c, kernel, kernel\n",
    "            G = G.repeat(1, c, 1, 1).to(DEVICE)\n",
    "            grid = torch.zeros(n, kernel_size, kernel_size, 2).to(DEVICE)\n",
    "            for gy in range(kernel_size):\n",
    "                for gx in range(kernel_size):\n",
    "                    grid[:, gy, gx, 0:1] = 2 * (j + gx) / (w - 1) - 1\n",
    "                    grid[:, gy, gx, 1:] = 2 * (i + gy) / (h - 1) - 1\n",
    "            # n, c, kernel, kernel\n",
    "            patch = F.grid_sample(f_k, grid,  mode='bilinear', padding_mode='zeros', align_corners=True)\n",
    "            f_i[:,:, py, px] = torch.sum(G * patch, dim=(2, 3))\n",
    "\n",
    "    return f_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_k= torch.randn(64, 2048, 7, 7).to(DEVICE)\n",
    "flow = torch.randn(64, 2, 224, 224).to(DEVICE)\n",
    "f_i = feature_warp(f_k, flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2048, 7, 7])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.DataParallel(RAFT(args))\n",
    "model.load_state_dict(torch.load(args.model))\n",
    "\n",
    "model = model.module\n",
    "model.to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(10, 2048, 1, 1)\n",
    "f = torch.randn(10, 2048, 7, 7)\n",
    "a = f*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2048, 7, 7])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "\n",
    "class wrap(nn.Module):\n",
    "    def __init__(self, model : nn.Module) -> None:\n",
    "        super().__init__()\n",
    "        self.feature = nn.Sequential(*list(model.children())[:-2])\n",
    "        self.out_channels = 2048\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        return {\"0\" : x}\n",
    "        \n",
    "def get_faster_rcnn_headed(n_classes=91, ec_type='vagan', out_ch=256):\n",
    "        wrapped = wrap(resnet)\n",
    "        return FasterRCNN(\n",
    "        wrapped,\n",
    "        num_classes=n_classes,  # COCO数据集有91类（包括背景）\n",
    "        rpn_anchor_generator = AnchorGenerator(\n",
    "                sizes=((32, 64, 128, 256, 512),),\n",
    "                aspect_ratios=((0.5, 1.0, 2.0),) * 5\n",
    "        ),\n",
    "        box_roi_pool = MultiScaleRoIAlign(\n",
    "                featmap_names=['0'],\n",
    "                output_size=7,\n",
    "                sampling_ratio=2\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = get_faster_rcnn_headed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform\n",
      "backbone\n",
      "rpn\n",
      "roi_heads\n"
     ]
    }
   ],
   "source": [
    "for n, _ in net.named_children():\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(EmbeddingNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 512, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(512, 2048, kernel_size=1, stride=1, padding=0)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.squeeze(-1).squeeze(-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EmbeddingNetwork(2048)\n",
    "fm = torch.randn(64,2048,7,7)\n",
    "out = net(fm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2048])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 4 but got size 8 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m x1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m      2\u001b[0m x2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m torch\u001b[38;5;241m.\u001b[39mcat((x1,x2), \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 4 but got size 8 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "x1 = torch.randn(1,4,4)\n",
    "x2 = torch.randn(1,8,8)\n",
    "torch.cat((x1,x2), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, c, h, w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
